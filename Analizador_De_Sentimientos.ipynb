{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uw-QDWDXGGDX"
   },
   "source": [
    "# Análisis de Sentimientos mediante Redes neuronales\n",
    "- Universidad Politécnica\n",
    "- Máster Universitario en Inteligencia Artificial\n",
    "- Web Science\n",
    "- Enero 2020\n",
    "\n",
    "## Autores\n",
    "- Antonio Sejas Mustafá\n",
    "- Elena Saa Noblejas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UreIwZo7GGDa"
   },
   "source": [
    "# INTRODUCCIÓN\n",
    "\n",
    "El objetivo de esta práctica es realizar una aplicación implementando alguno de los métodos vistos en clase. Por tanto tuvimos que elegir entre Sistema de recomendación, Clasificación de documentos siguiendo Topic Models, Reconocimiento de Entidades o Análisis de Sentimiento.\n",
    "\n",
    "Nosotros decidimos desarrollar este último proyecto. De este modo, nuestro objetivo es realizar una aplicación que dado un texto sea capaz de identificar si caracter positivo o negativo.\n",
    "\n",
    "Más concretamente hemos decidido trabajar sobre un dataset ya conocido. El data set de reviews de películas de IMDB. En el siguiente apartado ampliamos la información sobre el data set y comentamos dónde está disponible para su descarga.\n",
    "\n",
    "Nuestros textos, como ya hemos comentado son reviews de películas, y el sentimiento será si una review le ha gustado a un usuario o no.\n",
    "\n",
    "**¿Pero cómo vamos a evaluar nuestra aplicación?**\n",
    "\n",
    "Un texto puede estar lleno de ambiguedad, incluso una misma review puede tener comentarios pariales de caracter positivo y otras críticas negativas. Nosotros vamos a ignorar estas situaciones y seguiremos el gold standard marcado por el artículo descrito en Maas, A. L. et. al. 2011 [1]\n",
    "\n",
    "En este artículo se describe que las reviews positivas son aquellas que tengan una nota de 7 estrellas o más. Y de forma equivalente las negativas son las que tengan asociada una valoración de 0 a 4 estrellas. De esta forma no se tienen en cuenta los textos más ambiguos o indecisos, las reseñas Neutrales.\n",
    "\n",
    "\n",
    "## El Dataset de IMDB\n",
    "Este data set ha sido realizado por los investigadores de Stanford autores del artículo original [1] .\n",
    "\n",
    "El data set original está disponible en: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "El data set cuenta con 50.000 reviews, que ellos utilizaron 25.000 para entrenamiento y 25.000 para testing. Nosotros para reducir complejidad solo usaremos una mitad, que hemos tratado previamente para eliminar caracteres raros y poner los textos en minúsculas.\n",
    "\n",
    "Además este dataset cuenta con un porcentaje equilibrado de reseñas. Siendo la mitad positivas y la otra mitad negativas.\n",
    "\n",
    "Cada review tiene una etiqueta que lo categoriza de positiva o negativamente.\n",
    "\n",
    "\n",
    "## Tecnologías utilizadas\n",
    "\n",
    "A continuación describimos la metodología que hemos seguido para el analizador de sentimientos. Cada uno de estos puntos corresopnde con una sección del código.\n",
    "\n",
    "**Limpieza del data set**\n",
    "\n",
    "El primer paso es analizar y hacer un tratamiento de los textos del dataset. En el área de procesamiento del lenguaje natural hay una gran cantidad de alternativas y posibilidades. Es posible realizar distintas representación de los textos del corpus, y una gran variedad de extracción de características.\n",
    "\n",
    "El planteamiento del analizador de sentimientos puede verse de alguna forma con un clasificador de textos, en el que se intenta clasificar un text (review) como positivo o negativo.\n",
    "\n",
    "Por este motivo todos los métodos de representación utiizados en PLN son válidos. Algunos de estos modelos son: bag of words, vector space model, tf-idf, topic model.\n",
    "\n",
    "Nosotros hemos decidido obtener una bolsa de palabras (bag of words), teniendo en cuenta la frecuencia relativa con respecto a la otra clase. La idea es similar a un TF-IDF pero a nivel de clase. Esta representación nos permitirá darle más peso a las palabras más polarizadas.\n",
    "\n",
    "Por mantener determinar un límite en esta práctica no aplicaremos ningún procedimiento lematización ni stemming. Tampoco utilizaremos n-gramas. Únicamente eliminaremos las palabras vacías, stopwords, para reducir el ruido de los datos. La tokenización utilizada consiste en convertir las palabas en índices de un array. Todo este tipo de técnicas las hemos visto en clase y también se describen en más detalle en el libro\"Natural Language Processing in Action\" [3]\n",
    "\n",
    "**Entrenamiento**\n",
    "\n",
    "De forma similar a la limipieza del dataset y la extracción de variables predictoras, en el entrenamiento podemos utilizar prácticamente cualquier algoritmo de clasificación. Desde un Naive Bayes, Support Vector Machine, árboles de decisión o redes neuronales son algunas de las opciones más utilizadas.\n",
    "\n",
    "Nosotros al no haber cursado ninguna asignatura de redes neuronales, hemos decidido utilizar una red neuronal, en concreto un Perceptrón multicapa.\n",
    "\n",
    "La primera capa, capa de entrada, tendrá tantos nodos como palabras haya en nuestro vocabulario. La segunda capa tendrá 30 nodos, de forma experimental hemos observado un buen comportamiento con 10 a 30 nodos.\n",
    "\n",
    "Por último la capa de salida tendrá un solo nodo que dará un valor comprendido entre 0 y 1. Cuanto más cerca del 1 , más positiva se considerará la reseña. Un valor cercano al 0.5 se considerará la reseña \"neutral\".\n",
    "\n",
    "\n",
    "**Validación**\n",
    "\n",
    "Por último, nosotros hemos preferido evaluar la precisión de nuestro algoritmo utilizando un dropout 70/30 por sencillez de implementación. Una solución más profesional requeriría utilizar métodos de validación más sofistiados como un k-fold.\n",
    "\n",
    "Además hemos observado que incluso usando la mitad del data set de entrenamiento, obtenemos valores muy cercanos al SVM descrito en el artículo [1]. En el artículo se alcanzan precisiones de entorno al 0.88, mientras que como veremos nuestra red se queda en 0.86 debido a falta de reducción de ruido comentada anteriormente.\n",
    "\n",
    "**Extra**\n",
    "\n",
    "De forma adicional, hemos creado una celda con una caja de texto para que se pueda comprobar el fucionamiento con textos fuera del data set. Esta caja de texto está identificada bajo el título \"Inserta un texto para probar el analizador de sentimientos\". Hay que escribir un texto y ejecutar esa celda y la siguiente para ver los resultados.\n",
    "\n",
    "- El código está autocontenido en este Jupyter Notebook. El cual está disponible online: https://colab.research.google.com/drive/11ZvUGrctfSbuTqa_tk3J-SNkEolqmZbK\n",
    "- Además el código fuente y el dataset están disponibles en Github: https://github.com/sejas/muia-imdb-sentiment-analysis\n",
    "\n",
    "\n",
    "## Referencias\n",
    "\n",
    "1. Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics. [Descargar Artículo](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)\n",
    "\n",
    "2. Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-term Memory. Neural computation. 9. 1735-80. 10.1162/neco.1997.9.8.1735. \n",
    "\n",
    "3. Lane, H., Howard, C., & Hapke, H. M. (2019). Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python. Manning Publications Company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQZwNR2pGGDa",
    "nbpresent": {
     "id": "56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"
    }
   },
   "source": [
    "# CARGA DEL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEEJbDOdvipr"
   },
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VvUHPCDQunY-",
    "outputId": "2e158ca4-3809-4e9f-bb95-90cf25b2c42a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>bromwell high is a cartoon comedy . it ran at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>brilliant over  acting by lesley ann warren . ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                                            reviews\n",
       "0  POSITIVE  bromwell high is a cartoon comedy . it ran at ...\n",
       "1  NEGATIVE  story of a man who has unnatural feelings for ...\n",
       "2  POSITIVE  homelessness  or houselessness as george carli...\n",
       "3  NEGATIVE  airport    starts as a brand new luxury    pla...\n",
       "4  POSITIVE  brilliant over  acting by lesley ann warren . ..."
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de los datos en un dataframe\n",
    "df = pd.read_csv('imdb.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SE77xCA5unO7",
    "outputId": "4510c2ee-f5ac-472d-85c2-5d2cff331142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JyMPaevGGDe"
   },
   "source": [
    "Los datos de `imdb.csv` han sido preprocesados y el contenido está preparado para contener solo caracteres en minúsculas. Esto es para simplificar el la identificación de las palabras, independientemente de cómo hayan sido escritas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0n_z-yoGGDl"
   },
   "source": [
    "# ANÁLISIS Y TRATAMIENTO PREVIO DEL DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jROrVMJUGGDq"
   },
   "source": [
    "Utilizando tres objetos `Counter` podemos calcular la frequencia absoluta para cada tipod e clase, positiva y negativa y un tercer counter para la contabilizar la frecuencia total de cada palabra en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsUL3NFiGGDr"
   },
   "outputs": [],
   "source": [
    "positive_freq = Counter()\n",
    "negative_freq = Counter()\n",
    "total_freq = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lFpZXr9GGDs"
   },
   "source": [
    "Además de contabilizar la frecuencia de cada palabra en cada clase y en total, aprovechamos para eliminar las palabras vacías previamente conocidas y facilitadas por sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzEL0AMW8uBZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "custom_stopwords = stop_words.ENGLISH_STOP_WORDS.union(['br', '.'])\n",
    "def remove_stopwords(text):\n",
    "  return [word for word in text.split(' ') if word not in custom_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdWtJeIYGGDt"
   },
   "outputs": [],
   "source": [
    "for _, (label, review) in df[df['labels']=='POSITIVE'].iterrows():\n",
    "  positive_freq += Counter(remove_stopwords(review))\n",
    "for _, (label, review) in df[df['labels']=='NEGATIVE'].iterrows():\n",
    "  negative_freq += Counter(remove_stopwords(review))\n",
    "\n",
    "total_freq = positive_freq + negative_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8pHK6zhGGDv"
   },
   "source": [
    "Extraemos las palabras de las reseñas positivas y negativas ordenándolas de más a menos comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "YGSFg9eXGGDv",
    "outputId": "b6414440-0c44-46a4-ab17-b597b9323135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 550468),\n",
       " ('s', 33815),\n",
       " ('film', 20937),\n",
       " ('movie', 19074),\n",
       " ('t', 13720),\n",
       " ('like', 9038),\n",
       " ('good', 7720),\n",
       " ('just', 7152),\n",
       " ('story', 6780),\n",
       " ('time', 6515),\n",
       " ('great', 6419),\n",
       " ('really', 5476),\n",
       " ('people', 4479),\n",
       " ('best', 4319),\n",
       " ('love', 4301),\n",
       " ('life', 4199),\n",
       " ('way', 4036),\n",
       " ('films', 3813),\n",
       " ('think', 3655),\n",
       " ('movies', 3586)]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "G-NLxaHkGGDx",
    "outputId": "c67b1ffc-3dc1-4392-ec8e-2809bba41356"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 561462),\n",
       " ('s', 31546),\n",
       " ('movie', 24965),\n",
       " ('t', 20361),\n",
       " ('film', 19218),\n",
       " ('like', 11238),\n",
       " ('just', 10619),\n",
       " ('good', 7423),\n",
       " ('bad', 7401),\n",
       " ('really', 6262),\n",
       " ('time', 6209),\n",
       " ('don', 5336),\n",
       " ('story', 5208),\n",
       " ('people', 4806),\n",
       " ('make', 4722),\n",
       " ('plot', 4154),\n",
       " ('movies', 4080),\n",
       " ('acting', 4056),\n",
       " ('way', 3989),\n",
       " ('think', 3643)]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQSHtcQDGGDz"
   },
   "source": [
    "Aunque hayamos quitado las palabras vacías de un diccionario, hay un gran número de palabras vacías intrínsecas a nuestro dominio. En nuestro caso estas palabras que no aportan valor a la hora de distinguir entre una polarización positivia o negativa deberían ser consideradas como palabras vacías. Un ejemplo de estas palabras son muchas de las que aparecen en las listas de arriba. film, movie, acting y muchos nombres de actores y películas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sj0SVQa6asqS"
   },
   "source": [
    "A continuación calculamos el ratio de las palabras positivas entre las negativas, esto nos indicará si una palabra es muy positiva, neutra o nada positiva. \n",
    "\n",
    "La forma de calcular este ratio de frecuencia es:\n",
    "`número de usos positivos / (número de usos negativos+1)`\n",
    "\n",
    "Se le añade `+1` al denominador para no dividir entre 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_05SOpb-GGDz"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 200\n",
    "positive_negative_prop = Counter()\n",
    "\n",
    "for word,freq in list(total_freq.most_common()):\n",
    "    if(freq > MIN_FREQ):\n",
    "        proportion = positive_freq[word] / float(negative_freq[word]+1)\n",
    "        positive_negative_prop[word] = proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoQYUq4TGGD1"
   },
   "source": [
    "Examinamos el ratio de algunas palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iYQ2_hHSGGD2",
    "outputId": "3bdc48b6-f8bb-485b-f262-d0ddf87f1a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'film' = 1.089390707112753\n",
      "Word 'fantastic' = 4.503448275862069\n",
      "Word 'bad' = 0.2576330721426641\n"
     ]
    }
   ],
   "source": [
    "def check_words(words_list):\n",
    "  for word_to_check in words_list:\n",
    "    print(\"Word '%s' = %s\"%(word_to_check, positive_negative_prop[word_to_check]))\n",
    "check_words(['film', 'fantastic', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5Uz7zUnGGD4"
   },
   "source": [
    "Como podemos ver, las palabras positivas tendrán valores muy altos. (>1)\n",
    "Las palabras neutrales que aparecen en reviews positivas o negativas, tendrán valores muy cercanos a 1. (Equilibradas)\n",
    "Y las palabras negativas estarán muy próximas a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmDAIyQrGGD4"
   },
   "outputs": [],
   "source": [
    "for word,ratio in positive_negative_prop.most_common():\n",
    "    positive_negative_prop[word] = np.log(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ST_0TWN1GGD6"
   },
   "source": [
    "Una forma sencilla de normalizar estos valores y conseguir que las palabras neutrales estén en torno al 0 en vez de entorno al 1, es usando la función logaritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3Eo_D-jGGD6"
   },
   "source": [
    "A continuación comprobamos las mismas palabras anterioremente comprobadas y observamos los nuevos valores normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jxVz_cDGGGD7",
    "outputId": "6a9ab8bf-0290-4849-eb48-a3915bf19dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'film' = 0.08561855565085673\n",
      "Word 'fantastic' = 1.5048433868558566\n",
      "Word 'bad' = -1.3562189073456823\n"
     ]
    }
   ],
   "source": [
    "check_words(['film', 'fantastic', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14VoDlASGGD8"
   },
   "source": [
    "Arriba vemos que film, apenas aporta un valor discriminatorio.\n",
    "\n",
    "A continuación vemos la lista de palabras más polarizadas y sus nuevos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "mZ4DiPvjGGD9",
    "outputId": "1652c70c-9a8b-40a3-9195-8a5db9cb8772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('victoria', 2.681021528714291),\n",
       " ('captures', 2.038619547159581),\n",
       " ('wonderfully', 2.0218960560332353),\n",
       " ('powell', 1.978345424808467),\n",
       " ('refreshing', 1.8551812956655511),\n",
       " ('delightful', 1.8002701588959635),\n",
       " ('beautifully', 1.7626953362841438),\n",
       " ('underrated', 1.7197859696029656),\n",
       " ('superb', 1.7091514458966952),\n",
       " ('welles', 1.667706820558076),\n",
       " ('sinatra', 1.6389967146756448),\n",
       " ('touching', 1.637217476541176),\n",
       " ('stewart', 1.611998733295774),\n",
       " ('brilliantly', 1.5950491749820008),\n",
       " ('friendship', 1.5677652160335325),\n",
       " ('wonderful', 1.5645425925262093),\n",
       " ('magnificent', 1.54663701119507),\n",
       " ('finest', 1.546259010812569),\n",
       " ('jackie', 1.5439233053234738),\n",
       " ('freedom', 1.5091151908062312)]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_negative_prop.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "K5SsbCvlGGD_",
    "outputId": "04e48df5-2ef7-4620-aa31-f432ba0c7ff0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unfunny', -2.6922395950755678),\n",
       " ('waste', -2.6193845640165536),\n",
       " ('pointless', -2.4553061800117097),\n",
       " ('redeeming', -2.3682390632154826),\n",
       " ('lousy', -2.307572634505085),\n",
       " ('worst', -2.286987896180378),\n",
       " ('laughable', -2.264363880173848),\n",
       " ('awful', -2.227194247027435),\n",
       " ('poorly', -2.2207550747464135),\n",
       " ('sucks', -1.987068221548821),\n",
       " ('lame', -1.981767458946166),\n",
       " ('insult', -1.978345424808467),\n",
       " ('horrible', -1.9102590939512902),\n",
       " ('amateurish', -1.9095425048844386),\n",
       " ('pathetic', -1.9003933102308506),\n",
       " ('wasted', -1.8382794848629478),\n",
       " ('crap', -1.8281271133989299),\n",
       " ('tedious', -1.802454758344803),\n",
       " ('dreadful', -1.7725281073001673),\n",
       " ('badly', -1.753626599532611)]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(positive_negative_prop.most_common()))[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfUimxTA5mlK"
   },
   "source": [
    "La aparición de \"Victoria\" parece indicar que sus películas tienen asociadas muy buenas críticas. Pero sabemos que en inglés hace referencia a un nombre propio, por lo que  para mejorar nuestra predicción habría considerar los nombres propios como stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "S32SHBs8MjRI",
    "outputId": "9fa2bfb6-1c83-4796-e2f3-050a075ce625"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffb79d86fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(positive_negative_prop.most_common()).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aU16k5-uc-ea"
   },
   "source": [
    "Este histograma nos enseña la polaridad de las palabras en todo el corpus. Podemos observar que sigue una distribución normal con media en torno al 0. Es decir, la mayoría de las palabras están categorizadas como neutrales. Esto es ruido en nuestro clasificador. Esto se podría solucionar teniendo en cuenta aquellas palabas que aporten un valor discriminatorio mayor de |0.5|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "8pe1hDdOGGEJ"
   },
   "source": [
    "# GENERANDO NUESTRO VOCABULARIO\n",
    "A continuación para \"tokenizar\" nuestros textos y convertirlos en vector de palabras, vamos a crear un vocabulario que será la entrada de nuestra red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjxYkpwaGGEK"
   },
   "outputs": [],
   "source": [
    "vocab = set(total_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6ntRQHhTGGEM",
    "outputId": "bccfe731-abba-44a4-890c-3dbf224e7243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73759\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NcLsGFlsGGEV",
    "outputId": "3b07bdf0-b9b2-4172-ecf3-35fa88920518"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'pork': 1,\n",
       " 'cancer': 2,\n",
       " 'hypermacho': 3,\n",
       " 'beam': 4,\n",
       " 'didja': 5,\n",
       " 'sires': 6,\n",
       " 'colonised': 7,\n",
       " 'jest': 8,\n",
       " 'fem': 9,\n",
       " 'mitochondrial': 10,\n",
       " 'azuma': 11,\n",
       " 'stunk': 12,\n",
       " 'attracting': 13,\n",
       " 'cathernine': 14,\n",
       " 'ventricle': 15,\n",
       " 'ding': 16,\n",
       " 'religous': 17,\n",
       " 'training': 18,\n",
       " 'cranks': 19,\n",
       " 'hobbs': 20,\n",
       " 'novac': 21,\n",
       " 'millennia': 22,\n",
       " 'zinn': 23,\n",
       " 'sacrilage': 24,\n",
       " 'mistry': 25,\n",
       " 'sensualists': 26,\n",
       " 'giff': 27,\n",
       " 'bungling': 28,\n",
       " 'raechel': 29,\n",
       " 'swedes': 30,\n",
       " 'miffed': 31,\n",
       " 'ultimate': 32,\n",
       " 'dought': 33,\n",
       " 'plagiaristic': 34,\n",
       " 'limned': 35,\n",
       " 'jee': 36,\n",
       " 'aracnophobia': 37,\n",
       " 'centerpiece': 38,\n",
       " 'unfaithal': 39,\n",
       " 'knievel': 40,\n",
       " 'ecstacy': 41,\n",
       " 'trudged': 42,\n",
       " 'alun': 43,\n",
       " 'habituation': 44,\n",
       " 'cannibalism': 45,\n",
       " 'alarmist': 46,\n",
       " 'looney': 47,\n",
       " 'sudser': 48,\n",
       " 'min': 49,\n",
       " 'michelle': 50,\n",
       " 'winninger': 51,\n",
       " 'deployment': 52,\n",
       " 'menzel': 53,\n",
       " 'demonstrative': 54,\n",
       " 'overpowered': 55,\n",
       " 'seema': 56,\n",
       " 'psychotics': 57,\n",
       " 'coughthe': 58,\n",
       " 'rollin': 59,\n",
       " 'interferring': 60,\n",
       " 'shimbei': 61,\n",
       " 'orientated': 62,\n",
       " 'traumatized': 63,\n",
       " 'meriwether': 64,\n",
       " 'kind': 65,\n",
       " 'gruff': 66,\n",
       " 'palsey': 67,\n",
       " 'substories': 68,\n",
       " 'acquittal': 69,\n",
       " 'movecheck': 70,\n",
       " 'compromised': 71,\n",
       " 'zarustica': 72,\n",
       " 'maadri': 73,\n",
       " 'kaiser': 74,\n",
       " 'budgetary': 75,\n",
       " 'mt': 76,\n",
       " 'factors': 77,\n",
       " 'goulding': 78,\n",
       " 'transposing': 79,\n",
       " 'chineese': 80,\n",
       " 'herbal': 81,\n",
       " 'orkly': 82,\n",
       " 'murderer': 83,\n",
       " 'stephan': 84,\n",
       " 'tage': 85,\n",
       " 'forefathers': 86,\n",
       " 'plays': 87,\n",
       " 'dysfunction': 88,\n",
       " 'gramophone': 89,\n",
       " 'pendleton': 90,\n",
       " 'juxtapositions': 91,\n",
       " 'upto': 92,\n",
       " 'excitement': 93,\n",
       " 'ruphert': 94,\n",
       " 'ultimo': 95,\n",
       " 'mallorquins': 96,\n",
       " 'lunacy': 97,\n",
       " 'pratfalls': 98,\n",
       " 'skyraiders': 99,\n",
       " 'varela': 100,\n",
       " 'rexes': 101,\n",
       " 'mattresses': 102,\n",
       " 'shvollenpecker': 103,\n",
       " 'oversexed': 104,\n",
       " 'taiwanese': 105,\n",
       " 'toyota': 106,\n",
       " 'neds': 107,\n",
       " 'sugarman': 108,\n",
       " 'facebuster': 109,\n",
       " 'doel': 110,\n",
       " 'veal': 111,\n",
       " 'druidic': 112,\n",
       " 'wary': 113,\n",
       " 'extravaganzas': 114,\n",
       " 'spiteful': 115,\n",
       " 'sublime': 116,\n",
       " 'nyfd': 117,\n",
       " 'enthuses': 118,\n",
       " 'wheaton': 119,\n",
       " 'pharmaceutical': 120,\n",
       " 'fulfill': 121,\n",
       " 'innocence': 122,\n",
       " 'undertake': 123,\n",
       " 'infantile': 124,\n",
       " 'crapfest': 125,\n",
       " 'nec': 126,\n",
       " 'shroyer': 127,\n",
       " 'flour': 128,\n",
       " 'valseuses': 129,\n",
       " 'text': 130,\n",
       " 'breasted': 131,\n",
       " 'tachigui': 132,\n",
       " 'additives': 133,\n",
       " 'vanlint': 134,\n",
       " 'mcphillip': 135,\n",
       " 'impersonated': 136,\n",
       " 'fictionalization': 137,\n",
       " 'hitler': 138,\n",
       " 'burry': 139,\n",
       " 'curses': 140,\n",
       " 'worn': 141,\n",
       " 'thirbly': 142,\n",
       " 'spitted': 143,\n",
       " 'calhoun': 144,\n",
       " 'hoyden': 145,\n",
       " 'peculiarities': 146,\n",
       " 'crops': 147,\n",
       " 'blinding': 148,\n",
       " 'gossemar': 149,\n",
       " 'genghis': 150,\n",
       " 'dusting': 151,\n",
       " 'mausoleum': 152,\n",
       " 'braincell': 153,\n",
       " 'carrer': 154,\n",
       " 'thumper': 155,\n",
       " 'wale': 156,\n",
       " 'beresford': 157,\n",
       " 'coleman': 158,\n",
       " 'deix': 159,\n",
       " 'porkys': 160,\n",
       " 'weasel': 161,\n",
       " 'norton': 162,\n",
       " 'garmes': 163,\n",
       " 'croquet': 164,\n",
       " 'aristocats': 165,\n",
       " 'cigliutti': 166,\n",
       " 'amore': 167,\n",
       " 'casket': 168,\n",
       " 'pending': 169,\n",
       " 'mutated': 170,\n",
       " 'probate': 171,\n",
       " 'favourable': 172,\n",
       " 'grandeurs': 173,\n",
       " 'cavelleri': 174,\n",
       " 'exasperated': 175,\n",
       " 'kak': 176,\n",
       " 'conflictive': 177,\n",
       " 'paradoxically': 178,\n",
       " 'aamir': 179,\n",
       " 'aauugghh': 180,\n",
       " 'onhand': 181,\n",
       " 'deshimaru': 182,\n",
       " 'strolls': 183,\n",
       " 'grete': 184,\n",
       " 'sickroom': 185,\n",
       " 'clouded': 186,\n",
       " 'baguettes': 187,\n",
       " 'unabsorbing': 188,\n",
       " 'sarajevo': 189,\n",
       " 'sulk': 190,\n",
       " 'chart': 191,\n",
       " 'explore': 192,\n",
       " 'permitted': 193,\n",
       " 'malkovichian': 194,\n",
       " 'whys': 195,\n",
       " 'schlitz': 196,\n",
       " 'disingenuous': 197,\n",
       " 'hustle': 198,\n",
       " 'immortel': 199,\n",
       " 'insightfully': 200,\n",
       " 'workforces': 201,\n",
       " 'lyndon': 202,\n",
       " 'aden': 203,\n",
       " 'dunham': 204,\n",
       " 'disbelieving': 205,\n",
       " 'dunbar': 206,\n",
       " 'segal': 207,\n",
       " 'laroche': 208,\n",
       " 'shakespearian': 209,\n",
       " 'peasant': 210,\n",
       " 'retention': 211,\n",
       " 'concerted': 212,\n",
       " 'serve': 213,\n",
       " 'getz': 214,\n",
       " 'discos': 215,\n",
       " 'fused': 216,\n",
       " 'looong': 217,\n",
       " 'deceiving': 218,\n",
       " 'ancients': 219,\n",
       " 'brigadier': 220,\n",
       " 'sistahs': 221,\n",
       " 'violin': 222,\n",
       " 'unengineered': 223,\n",
       " 'deranged': 224,\n",
       " 'lachlin': 225,\n",
       " 'veoh': 226,\n",
       " 'clung': 227,\n",
       " 'ran': 228,\n",
       " 'swabby': 229,\n",
       " 'rataud': 230,\n",
       " 'endearment': 231,\n",
       " 'comity': 232,\n",
       " 'bookend': 233,\n",
       " 'waaaaaayyyy': 234,\n",
       " 'siren': 235,\n",
       " 'misleads': 236,\n",
       " 'alrite': 237,\n",
       " 'examination': 238,\n",
       " 'panned': 239,\n",
       " 'themsleves': 240,\n",
       " 'wandered': 241,\n",
       " 'simper': 242,\n",
       " 'pliers': 243,\n",
       " 'rump': 244,\n",
       " 'cripplingly': 245,\n",
       " 'scrawl': 246,\n",
       " 'lewinski': 247,\n",
       " 'gearheads': 248,\n",
       " 'ktla': 249,\n",
       " 'ambience': 250,\n",
       " 'dozens': 251,\n",
       " 'presumes': 252,\n",
       " 'awards': 253,\n",
       " 'surpressors': 254,\n",
       " 'edits': 255,\n",
       " 'difficulties': 256,\n",
       " 'remar': 257,\n",
       " 'wheelchairs': 258,\n",
       " 'fiascos': 259,\n",
       " 'claimed': 260,\n",
       " 'waldeman': 261,\n",
       " 'dangles': 262,\n",
       " 'aloud': 263,\n",
       " 'luncheon': 264,\n",
       " 'cliffhangers': 265,\n",
       " 'reminding': 266,\n",
       " 'protected': 267,\n",
       " 'serafinowicz': 268,\n",
       " 'sorrell': 269,\n",
       " 'bused': 270,\n",
       " 'vulnerability': 271,\n",
       " 'kaleidoscope': 272,\n",
       " 'lizard': 273,\n",
       " 'plateful': 274,\n",
       " 'subbed': 275,\n",
       " 'mpkdh': 276,\n",
       " 'majkowski': 277,\n",
       " 'eroticism': 278,\n",
       " 'latecomers': 279,\n",
       " 'outreach': 280,\n",
       " 'visualizes': 281,\n",
       " 'ramotswe': 282,\n",
       " 'scientific': 283,\n",
       " 'mcgaw': 284,\n",
       " 'zb': 285,\n",
       " 'mole': 286,\n",
       " 'macho': 287,\n",
       " 'uninstructive': 288,\n",
       " 'resourceful': 289,\n",
       " 'pumba': 290,\n",
       " 'soleil': 291,\n",
       " 'whopper': 292,\n",
       " 'adhering': 293,\n",
       " 'slobber': 294,\n",
       " 'ai': 295,\n",
       " 'lifelike': 296,\n",
       " 'finisher': 297,\n",
       " 'eponymous': 298,\n",
       " 'shoudln': 299,\n",
       " 'oyl': 300,\n",
       " 'carrefour': 301,\n",
       " 'argonne': 302,\n",
       " 'golovanov': 303,\n",
       " 'gunmen': 304,\n",
       " 'palestinians': 305,\n",
       " 'precocious': 306,\n",
       " 'teapot': 307,\n",
       " 'somtimes': 308,\n",
       " 'aiden': 309,\n",
       " 'curmudgeon': 310,\n",
       " 'opting': 311,\n",
       " 'imagery': 312,\n",
       " 'stitches': 313,\n",
       " 'irresistibly': 314,\n",
       " 'ezra': 315,\n",
       " 'hypesters': 316,\n",
       " 'spritely': 317,\n",
       " 'honeymooners': 318,\n",
       " 'mined': 319,\n",
       " 'muggings': 320,\n",
       " 'fallow': 321,\n",
       " 'grimm': 322,\n",
       " 'fiddler': 323,\n",
       " 'daneille': 324,\n",
       " 'carelessness': 325,\n",
       " 'braveheart': 326,\n",
       " 'cahoots': 327,\n",
       " 'reflexivity': 328,\n",
       " 'agekudos': 329,\n",
       " 'abdu': 330,\n",
       " 'tick': 331,\n",
       " 'kindling': 332,\n",
       " 'flowed': 333,\n",
       " 'terrifically': 334,\n",
       " 'montegna': 335,\n",
       " 'rest': 336,\n",
       " 'unperceptive': 337,\n",
       " 'fannin': 338,\n",
       " 'hindersome': 339,\n",
       " 'monique': 340,\n",
       " 'einstein': 341,\n",
       " 'lea': 342,\n",
       " 'portrayed': 343,\n",
       " 'garrett': 344,\n",
       " 'arcaica': 345,\n",
       " 'parlor': 346,\n",
       " 'blight': 347,\n",
       " 'abusing': 348,\n",
       " 'gainful': 349,\n",
       " 'infects': 350,\n",
       " 'twiggy': 351,\n",
       " 'storszek': 352,\n",
       " 'tediousness': 353,\n",
       " 'tigerland': 354,\n",
       " 'spirited': 355,\n",
       " 'skipping': 356,\n",
       " 'gills': 357,\n",
       " 'barrels': 358,\n",
       " 'soni': 359,\n",
       " 'guanajuato': 360,\n",
       " 'burkhalter': 361,\n",
       " 'ingela': 362,\n",
       " 'emulations': 363,\n",
       " 'estefan': 364,\n",
       " 'adlai': 365,\n",
       " 'trainor': 366,\n",
       " 'attraction': 367,\n",
       " 'adma': 368,\n",
       " 'flippantly': 369,\n",
       " 'irritated': 370,\n",
       " 'pendant': 371,\n",
       " 'annoyed': 372,\n",
       " 'storaro': 373,\n",
       " 'az': 374,\n",
       " 'punters': 375,\n",
       " 'radical': 376,\n",
       " 'unresponsive': 377,\n",
       " 'printer': 378,\n",
       " 'hmmmmmmmm': 379,\n",
       " 'portrayer': 380,\n",
       " 'gained': 381,\n",
       " 'lars': 382,\n",
       " 'willed': 383,\n",
       " 'appreciation': 384,\n",
       " 'herilhy': 385,\n",
       " 'campy': 386,\n",
       " 'fahrenheit': 387,\n",
       " 'rodrix': 388,\n",
       " 'nordham': 389,\n",
       " 'underfoot': 390,\n",
       " 'woolgathering': 391,\n",
       " 'bs': 392,\n",
       " 'aldonova': 393,\n",
       " 'elequence': 394,\n",
       " 'suspending': 395,\n",
       " 'incubates': 396,\n",
       " 'sans': 397,\n",
       " 'misfire': 398,\n",
       " 'reassuring': 399,\n",
       " 'jerri': 400,\n",
       " 'rework': 401,\n",
       " 'utilities': 402,\n",
       " 'handlers': 403,\n",
       " 'margineanus': 404,\n",
       " 'cos': 405,\n",
       " 'masters': 406,\n",
       " 'widened': 407,\n",
       " 'excuse': 408,\n",
       " 'pinkish': 409,\n",
       " 'split': 410,\n",
       " 'kewl': 411,\n",
       " 'attract': 412,\n",
       " 'wavy': 413,\n",
       " 'alda': 414,\n",
       " 'recognizable': 415,\n",
       " 'whip': 416,\n",
       " 'securing': 417,\n",
       " 'insular': 418,\n",
       " 'idiosyncratic': 419,\n",
       " 'hayseed': 420,\n",
       " 'tukur': 421,\n",
       " 'advisedly': 422,\n",
       " 'proposal': 423,\n",
       " 'espeically': 424,\n",
       " 'astrotech': 425,\n",
       " 'shoufukutei': 426,\n",
       " 'muncie': 427,\n",
       " 'notoriety': 428,\n",
       " 'escapism': 429,\n",
       " 'outburst': 430,\n",
       " 'hipper': 431,\n",
       " 'condon': 432,\n",
       " 'prix': 433,\n",
       " 'glop': 434,\n",
       " 'lespart': 435,\n",
       " 'occupational': 436,\n",
       " 'slacken': 437,\n",
       " 'kerkhof': 438,\n",
       " 'gymnasts': 439,\n",
       " 'rigorous': 440,\n",
       " 'jame': 441,\n",
       " 'definetly': 442,\n",
       " 'someway': 443,\n",
       " 'caresses': 444,\n",
       " 'deepak': 445,\n",
       " 'sutdying': 446,\n",
       " 'da': 447,\n",
       " 'groundwork': 448,\n",
       " 'ford': 449,\n",
       " 'pentimento': 450,\n",
       " 'hanns': 451,\n",
       " 'drab': 452,\n",
       " 'der': 453,\n",
       " 'underwear': 454,\n",
       " 'casper': 455,\n",
       " 'puppetry': 456,\n",
       " 'pakis': 457,\n",
       " 'pearlman': 458,\n",
       " 'bets': 459,\n",
       " 'deservingly': 460,\n",
       " 'hesitates': 461,\n",
       " 'liberty': 462,\n",
       " 'inconvenience': 463,\n",
       " 'grosbard': 464,\n",
       " 'steam': 465,\n",
       " 'mounts': 466,\n",
       " 'warnercolor': 467,\n",
       " 'matt': 468,\n",
       " 'beatific': 469,\n",
       " 'colwell': 470,\n",
       " 'slumping': 471,\n",
       " 'doings': 472,\n",
       " 'miswrote': 473,\n",
       " 'jodoworsky': 474,\n",
       " 'floods': 475,\n",
       " 'enticement': 476,\n",
       " 'rigueur': 477,\n",
       " 'starsky': 478,\n",
       " 'nick': 479,\n",
       " 'monumentous': 480,\n",
       " 'naffness': 481,\n",
       " 'scratched': 482,\n",
       " 'mays': 483,\n",
       " 'starblazers': 484,\n",
       " 'doves': 485,\n",
       " 'wellpaced': 486,\n",
       " 'growls': 487,\n",
       " 'mist': 488,\n",
       " 'ropes': 489,\n",
       " 'baltimoreans': 490,\n",
       " 'touch': 491,\n",
       " 'aja': 492,\n",
       " 'valga': 493,\n",
       " 'recur': 494,\n",
       " 'contreras': 495,\n",
       " 'unbearded': 496,\n",
       " 'cassetti': 497,\n",
       " 'cascading': 498,\n",
       " 'megapack': 499,\n",
       " 'bandido': 500,\n",
       " 'sprays': 501,\n",
       " 'smuttiness': 502,\n",
       " 'ladder': 503,\n",
       " 'dosage': 504,\n",
       " 'milwall': 505,\n",
       " 'competent': 506,\n",
       " 'hilltop': 507,\n",
       " 'discomfort': 508,\n",
       " 'stutter': 509,\n",
       " 'draughtswoman': 510,\n",
       " 'stockpile': 511,\n",
       " 'littlekuriboh': 512,\n",
       " 'bootie': 513,\n",
       " 'disappoints': 514,\n",
       " 'koz': 515,\n",
       " 'proceeded': 516,\n",
       " 'solimeno': 517,\n",
       " 'avian': 518,\n",
       " 'wicked': 519,\n",
       " 'scales': 520,\n",
       " 'howls': 521,\n",
       " 'pleasaunces': 522,\n",
       " 'shead': 523,\n",
       " 'wickerman': 524,\n",
       " 'xylophonist': 525,\n",
       " 'companys': 526,\n",
       " 'lorado': 527,\n",
       " 'undertook': 528,\n",
       " 'utopia': 529,\n",
       " 'chihiro': 530,\n",
       " 'courtesan': 531,\n",
       " 'democratically': 532,\n",
       " 'broad': 533,\n",
       " 'conniving': 534,\n",
       " 'photographic': 535,\n",
       " 'davidbathsheba': 536,\n",
       " 'glum': 537,\n",
       " 'militaries': 538,\n",
       " 'unfairly': 539,\n",
       " 'ohio': 540,\n",
       " 'talosian': 541,\n",
       " 'grafted': 542,\n",
       " 'cof': 543,\n",
       " 'evers': 544,\n",
       " 'bogglingly': 545,\n",
       " 'overheating': 546,\n",
       " 'mammothly': 547,\n",
       " 'unfurnished': 548,\n",
       " 'loves': 549,\n",
       " 'battle': 550,\n",
       " 'qi': 551,\n",
       " 'tragedy': 552,\n",
       " 'blonde': 553,\n",
       " 'dystopic': 554,\n",
       " 'cineasts': 555,\n",
       " 'antonius': 556,\n",
       " 'tarka': 557,\n",
       " 'bloodthirst': 558,\n",
       " 'milieu': 559,\n",
       " 'vivant': 560,\n",
       " 'censured': 561,\n",
       " 'stinkpile': 562,\n",
       " 'differential': 563,\n",
       " 'affirmation': 564,\n",
       " 'lydia': 565,\n",
       " 'superlivemation': 566,\n",
       " 'financially': 567,\n",
       " 'pac': 568,\n",
       " 'funiest': 569,\n",
       " 'revolving': 570,\n",
       " 'applauds': 571,\n",
       " 'sperr': 572,\n",
       " 'sybil': 573,\n",
       " 'pedestrians': 574,\n",
       " 'promise': 575,\n",
       " 'elam': 576,\n",
       " 'gazongas': 577,\n",
       " 'categorised': 578,\n",
       " 'tura': 579,\n",
       " 'jeb': 580,\n",
       " 'opportune': 581,\n",
       " 'furgusson': 582,\n",
       " 'irl': 583,\n",
       " 'refuge': 584,\n",
       " 'enacting': 585,\n",
       " 'disenchantment': 586,\n",
       " 'tis': 587,\n",
       " 'breads': 588,\n",
       " 'transposed': 589,\n",
       " 'sivan': 590,\n",
       " 'johan': 591,\n",
       " 'siu': 592,\n",
       " 'beswick': 593,\n",
       " 'vlkava': 594,\n",
       " 'auburn': 595,\n",
       " 'gurl': 596,\n",
       " 'figuring': 597,\n",
       " 'numbingly': 598,\n",
       " 'soft': 599,\n",
       " 'centred': 600,\n",
       " 'harrowed': 601,\n",
       " 'hearkens': 602,\n",
       " 'joeseph': 603,\n",
       " 'moovies': 604,\n",
       " 'witchie': 605,\n",
       " 'cigs': 606,\n",
       " 'stage': 607,\n",
       " 'mitevska': 608,\n",
       " 'roulette': 609,\n",
       " 'rolly': 610,\n",
       " 'ramchand': 611,\n",
       " 'mulit': 612,\n",
       " 'ameteurish': 613,\n",
       " 'supplicant': 614,\n",
       " 'compositor': 615,\n",
       " 'pointer': 616,\n",
       " 'dooooosie': 617,\n",
       " 'rembrandt': 618,\n",
       " 'skolimowski': 619,\n",
       " 'vangelis': 620,\n",
       " 'dzundza': 621,\n",
       " 'cherri': 622,\n",
       " 'harvested': 623,\n",
       " 'filmmakers': 624,\n",
       " 'essendon': 625,\n",
       " 'nicolie': 626,\n",
       " 'reassigned': 627,\n",
       " 'calvins': 628,\n",
       " 'refinery': 629,\n",
       " 'amrish': 630,\n",
       " 'lesson': 631,\n",
       " 'nris': 632,\n",
       " 'clerical': 633,\n",
       " 'oooo': 634,\n",
       " 'medication': 635,\n",
       " 'phenomenons': 636,\n",
       " 'santoni': 637,\n",
       " 'moronfest': 638,\n",
       " 'soviet': 639,\n",
       " 'harden': 640,\n",
       " 'relationsip': 641,\n",
       " 'roofer': 642,\n",
       " 'afar': 643,\n",
       " 'neptune': 644,\n",
       " 'unforgetable': 645,\n",
       " 'sorcha': 646,\n",
       " 'ditz': 647,\n",
       " 'mehemet': 648,\n",
       " 'advice': 649,\n",
       " 'romantisised': 650,\n",
       " 'ulcerating': 651,\n",
       " 'millimeter': 652,\n",
       " 'snorer': 653,\n",
       " 'glady': 654,\n",
       " 'daylights': 655,\n",
       " 'anorexia': 656,\n",
       " 'gettysburg': 657,\n",
       " 'foe': 658,\n",
       " 'suck': 659,\n",
       " 'ising': 660,\n",
       " 'johar': 661,\n",
       " 'cradled': 662,\n",
       " 'womennone': 663,\n",
       " 'clampets': 664,\n",
       " 'ishwar': 665,\n",
       " 'dandies': 666,\n",
       " 'jughead': 667,\n",
       " 'themself': 668,\n",
       " 'chundering': 669,\n",
       " 'shipment': 670,\n",
       " 'owed': 671,\n",
       " 'wrestlemanias': 672,\n",
       " 'commercisliation': 673,\n",
       " 'vooren': 674,\n",
       " 'shipped': 675,\n",
       " 'brogues': 676,\n",
       " 'nectar': 677,\n",
       " 'kitties': 678,\n",
       " 'buyer': 679,\n",
       " 'tapers': 680,\n",
       " 'leidner': 681,\n",
       " 'perverted': 682,\n",
       " 'vaticani': 683,\n",
       " 'insouciance': 684,\n",
       " 'iannaccone': 685,\n",
       " 'succulently': 686,\n",
       " 'apprehending': 687,\n",
       " 'mitchel': 688,\n",
       " 'workday': 689,\n",
       " 'titty': 690,\n",
       " 'oppenheimer': 691,\n",
       " 'eser': 692,\n",
       " 'tassel': 693,\n",
       " 'sumptuousness': 694,\n",
       " 'intonations': 695,\n",
       " 'cherubic': 696,\n",
       " 'franklin': 697,\n",
       " 'propane': 698,\n",
       " 'senegalese': 699,\n",
       " 'compiled': 700,\n",
       " 'arret': 701,\n",
       " 'intrusively': 702,\n",
       " 'wrinkle': 703,\n",
       " 'urmila': 704,\n",
       " 'buds': 705,\n",
       " 'librarians': 706,\n",
       " 'cubbyholes': 707,\n",
       " 'portends': 708,\n",
       " 'interconnecting': 709,\n",
       " 'posterity': 710,\n",
       " 'norseman': 711,\n",
       " 'episodic': 712,\n",
       " 'bleating': 713,\n",
       " 'frumpy': 714,\n",
       " 'ofcourse': 715,\n",
       " 'rouged': 716,\n",
       " 'voerhoven': 717,\n",
       " 'stun': 718,\n",
       " 'beret': 719,\n",
       " 'scrutinized': 720,\n",
       " 'sequenes': 721,\n",
       " 'inhumanity': 722,\n",
       " 'merkle': 723,\n",
       " 'vomitum': 724,\n",
       " 'gobbler': 725,\n",
       " 'plastique': 726,\n",
       " 'frownbuster': 727,\n",
       " 'turaqui': 728,\n",
       " 'sanju': 729,\n",
       " 'x': 730,\n",
       " 'chakraborty': 731,\n",
       " 'curator': 732,\n",
       " 'strategies': 733,\n",
       " 'orientals': 734,\n",
       " 'poorly': 735,\n",
       " 'glass': 736,\n",
       " 'fellowship': 737,\n",
       " 'spaz': 738,\n",
       " 'decomp': 739,\n",
       " 'warbler': 740,\n",
       " 'aonghas': 741,\n",
       " 'withouts': 742,\n",
       " 'bergqvist': 743,\n",
       " 'dutt': 744,\n",
       " 'maclaine': 745,\n",
       " 'prowls': 746,\n",
       " 'millie': 747,\n",
       " 'turbulent': 748,\n",
       " 'clunks': 749,\n",
       " 'shards': 750,\n",
       " 'conaughey': 751,\n",
       " 'pounced': 752,\n",
       " 'lineal': 753,\n",
       " 'justicia': 754,\n",
       " 'ksm': 755,\n",
       " 'parnell': 756,\n",
       " 'alcoholic': 757,\n",
       " 'seafood': 758,\n",
       " 'marienbad': 759,\n",
       " 'mander': 760,\n",
       " 'rowdy': 761,\n",
       " 'designates': 762,\n",
       " 'cheerless': 763,\n",
       " 'hallgren': 764,\n",
       " 'bastidge': 765,\n",
       " 'aubrey': 766,\n",
       " 'panoramas': 767,\n",
       " 'ke': 768,\n",
       " 'blige': 769,\n",
       " 'nicks': 770,\n",
       " 'taunts': 771,\n",
       " 'thingie': 772,\n",
       " 'zerifferelli': 773,\n",
       " 'fisticuff': 774,\n",
       " 'dakota': 775,\n",
       " 'stettner': 776,\n",
       " 'relaxers': 777,\n",
       " 'cared': 778,\n",
       " 'entrenchments': 779,\n",
       " 'jaipur': 780,\n",
       " 'rosco': 781,\n",
       " 'murkily': 782,\n",
       " 'karogi': 783,\n",
       " 'sharpe': 784,\n",
       " 'msb': 785,\n",
       " 'kelemen': 786,\n",
       " 'anal': 787,\n",
       " 'entities': 788,\n",
       " 'hagerthy': 789,\n",
       " 'hyderabadi': 790,\n",
       " 'indulgent': 791,\n",
       " 'chicatillo': 792,\n",
       " 'capabilities': 793,\n",
       " 'leguizamo': 794,\n",
       " 'couleur': 795,\n",
       " 'apostrophe': 796,\n",
       " 'uncynical': 797,\n",
       " 'sadomasochism': 798,\n",
       " 'retreated': 799,\n",
       " 'kimmell': 800,\n",
       " 'artistry': 801,\n",
       " 'helen': 802,\n",
       " 'stagnation': 803,\n",
       " 'globalizing': 804,\n",
       " 'puh': 805,\n",
       " 'prosaically': 806,\n",
       " 'redhead': 807,\n",
       " 'footsteps': 808,\n",
       " 'longtime': 809,\n",
       " 'axiomatic': 810,\n",
       " 'fans': 811,\n",
       " 'xtianity': 812,\n",
       " 'alucard': 813,\n",
       " 'predominant': 814,\n",
       " 'lynchings': 815,\n",
       " 'fielding': 816,\n",
       " 'contessa': 817,\n",
       " 'fried': 818,\n",
       " 'abortive': 819,\n",
       " 'underscored': 820,\n",
       " 'adroitly': 821,\n",
       " 'awkwardness': 822,\n",
       " 'sinese': 823,\n",
       " 'travelcard': 824,\n",
       " 'maryam': 825,\n",
       " 'intact': 826,\n",
       " 'ads': 827,\n",
       " 'northam': 828,\n",
       " 'nafta': 829,\n",
       " 'matlock': 830,\n",
       " 'madchen': 831,\n",
       " 'swung': 832,\n",
       " 'numero': 833,\n",
       " 'genetics': 834,\n",
       " 'ashley': 835,\n",
       " 'scot': 836,\n",
       " 'zeffirelli': 837,\n",
       " 'slayers': 838,\n",
       " 'duquenne': 839,\n",
       " 'quibble': 840,\n",
       " 'fumes': 841,\n",
       " 'zues': 842,\n",
       " 'pap': 843,\n",
       " 'lasciviousness': 844,\n",
       " 'cukor': 845,\n",
       " 'lemuria': 846,\n",
       " 'pejorative': 847,\n",
       " 'toto': 848,\n",
       " 'midway': 849,\n",
       " 'vadis': 850,\n",
       " 'sliced': 851,\n",
       " 'businesspeople': 852,\n",
       " 'homey': 853,\n",
       " 'artisticly': 854,\n",
       " 'refracted': 855,\n",
       " 'dysfunctions': 856,\n",
       " 'atlanteans': 857,\n",
       " 'baby': 858,\n",
       " 'bassis': 859,\n",
       " 'reconstructions': 860,\n",
       " 'johannesburg': 861,\n",
       " 'jaret': 862,\n",
       " 'hungarian': 863,\n",
       " 'useless': 864,\n",
       " 'indestructible': 865,\n",
       " 'jacuzzi': 866,\n",
       " 'kayyyy': 867,\n",
       " 'mi': 868,\n",
       " 'milinkovic': 869,\n",
       " 'dioz': 870,\n",
       " 'discombobulation': 871,\n",
       " 'abm': 872,\n",
       " 'vise': 873,\n",
       " 'lovesick': 874,\n",
       " 'faraway': 875,\n",
       " 'unheated': 876,\n",
       " 'bogie': 877,\n",
       " 'messmer': 878,\n",
       " 'foreseeing': 879,\n",
       " 'labouf': 880,\n",
       " 'phoenicia': 881,\n",
       " 'overhears': 882,\n",
       " 'remunda': 883,\n",
       " 'unraveling': 884,\n",
       " 'daisies': 885,\n",
       " 'aristide': 886,\n",
       " 'bedingfield': 887,\n",
       " 'cheesecake': 888,\n",
       " 'terrace': 889,\n",
       " 'flagship': 890,\n",
       " 'pickup': 891,\n",
       " 'escalating': 892,\n",
       " 'uttara': 893,\n",
       " 'gunshots': 894,\n",
       " 'meres': 895,\n",
       " 'savales': 896,\n",
       " 'horrorfilm': 897,\n",
       " 'sheeple': 898,\n",
       " 'twine': 899,\n",
       " 'aboriginies': 900,\n",
       " 'hoydenish': 901,\n",
       " 'reshipping': 902,\n",
       " 'wouln': 903,\n",
       " 'speckle': 904,\n",
       " 'befuddled': 905,\n",
       " 'liebe': 906,\n",
       " 'mopey': 907,\n",
       " 'steffen': 908,\n",
       " 'noises': 909,\n",
       " 'install': 910,\n",
       " 'barren': 911,\n",
       " 'workaholics': 912,\n",
       " 'alcoholism': 913,\n",
       " 'bashing': 914,\n",
       " 'ilu': 915,\n",
       " 'disrupts': 916,\n",
       " 'republics': 917,\n",
       " 'briliant': 918,\n",
       " 'rheubottom': 919,\n",
       " 'eludes': 920,\n",
       " 'endearing': 921,\n",
       " 'alexanderplatz': 922,\n",
       " 'judgment': 923,\n",
       " 'colorfully': 924,\n",
       " 'darlene': 925,\n",
       " 'buckaroo': 926,\n",
       " 'machettes': 927,\n",
       " 'moncia': 928,\n",
       " 'gaita': 929,\n",
       " 'doctresses': 930,\n",
       " 'thunderbolt': 931,\n",
       " 'flak': 932,\n",
       " 'vanquishes': 933,\n",
       " 'supermen': 934,\n",
       " 'shuddup': 935,\n",
       " 'pinkie': 936,\n",
       " 'sensations': 937,\n",
       " 'elvira': 938,\n",
       " 'guessed': 939,\n",
       " 'consults': 940,\n",
       " 'amatuerish': 941,\n",
       " 'corsair': 942,\n",
       " 'munitions': 943,\n",
       " 'git': 944,\n",
       " 'hectic': 945,\n",
       " 'septic': 946,\n",
       " 'flapper': 947,\n",
       " 'atherton': 948,\n",
       " 'anons': 949,\n",
       " 'motos': 950,\n",
       " 'bambou': 951,\n",
       " 'childish': 952,\n",
       " 'reviczky': 953,\n",
       " 'graystone': 954,\n",
       " 'mandate': 955,\n",
       " 'heightens': 956,\n",
       " 'petron': 957,\n",
       " 'rods': 958,\n",
       " 'excell': 959,\n",
       " 'collection': 960,\n",
       " 'macrae': 961,\n",
       " 'wiping': 962,\n",
       " 'delli': 963,\n",
       " 'poltergeist': 964,\n",
       " 'minutia': 965,\n",
       " 'malikka': 966,\n",
       " 'upbringings': 967,\n",
       " 'noisier': 968,\n",
       " 'ifyou': 969,\n",
       " 'manchu': 970,\n",
       " 'prophets': 971,\n",
       " 'mice': 972,\n",
       " 'leit': 973,\n",
       " 'morrocco': 974,\n",
       " 'korman': 975,\n",
       " 'reviving': 976,\n",
       " 'slowed': 977,\n",
       " 'epstein': 978,\n",
       " 'testified': 979,\n",
       " 'bassett': 980,\n",
       " 'bendan': 981,\n",
       " 'punkris': 982,\n",
       " 'scolded': 983,\n",
       " 'okinawan': 984,\n",
       " 'poisoning': 985,\n",
       " 'blueprints': 986,\n",
       " 'impalement': 987,\n",
       " 'bethune': 988,\n",
       " 'ted': 989,\n",
       " 'dissertations': 990,\n",
       " 'oops': 991,\n",
       " 'deesh': 992,\n",
       " 'chaya': 993,\n",
       " 'gunnerside': 994,\n",
       " 'mano': 995,\n",
       " 'advision': 996,\n",
       " 'negro': 997,\n",
       " 'postino': 998,\n",
       " 'dumbed': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index = {}\n",
    "for i,word in enumerate(vocab):  \n",
    "    word2index[word] = i\n",
    "word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxJiAPATGGEk"
   },
   "source": [
    "# CLASIFICADOR MEDIANTE RED NEURONAL\n",
    "\n",
    "Ya nos acercamos al final de nuestro analizador de sentimientos. Hemos decidido utilizar una red neuronal clásica, que como ya hemos comentado en la introducción posee 3 capas. La primera que es de entradas y tiene la longitud de nuestro vocabulario. Cada review se codificará como un vector en el que cada cada elemento representa la frequencia de apariciones de esa palabra en el texto. La capa intermedia tiene 30 nodos, y finalmente la capa final tiene un solo nodo de salida.\n",
    "\n",
    "Nuestro clasificador puede en modo entrenamiento y test devuelve 2 etiquetas Positivo o Negativo. Y en modo interactivo (logger), además devuelve el nivel de confianza y una tercera etiqueta Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdPe5pUMGGFw"
   },
   "outputs": [],
   "source": [
    "class SentimentReviewClassifier:\n",
    "    def __init__(self, learning_rate = 0.01):\n",
    "        np.random.seed(7)\n",
    "        self.input_nodes = len(vocab)\n",
    "        self.middle_nodes = 30\n",
    "        self.final_nodes = 1\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize net\n",
    "        self.hidden_0_1 = np.zeros((self.input_nodes,self.middle_nodes))\n",
    "        self.hidden_1_2 = np.random.normal(0.0, self.middle_nodes**-0.5, \n",
    "                                                (self.middle_nodes, self.final_nodes))\n",
    "        self.first_layer = np.zeros((1,self.middle_nodes))\n",
    "    \n",
    "    translate_label = {\n",
    "        'POSITIVE': 1,\n",
    "        'NEGATIVE': 0,\n",
    "    }\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    def show_progress(self, i, total, correct):\n",
    "        progress =  str(100 * i/float(total))[:4]\n",
    "        accuracy =  str(correct * 100 / float(i+1))[:4]\n",
    "        sys.stdout.write(\"\\r - Progress:%s %%| Correct:%s | Accuracy:%s%%\"%(progress, correct, accuracy))\n",
    "        \n",
    "    def train(self, reviews_corpus, labels_t):\n",
    "        \"\"\" Update weights from corpus\"\"\"\n",
    "        reviews = list()\n",
    "        for review in reviews_corpus:\n",
    "            indices = set()\n",
    "            for word in remove_stopwords(review):\n",
    "                if(word in word2index.keys()):\n",
    "                    indices.add(word2index[word])\n",
    "            reviews.append(list(indices))\n",
    "\n",
    "        correct = 0\n",
    "        for i in range(len(reviews)):\n",
    "            review = reviews[i]\n",
    "            label = labels_t[i]\n",
    "            # Training\n",
    "            self.first_layer *= 0\n",
    "            for index in review:\n",
    "                self.first_layer += self.hidden_0_1[index]\n",
    "            second_layer = self.sigmoid(self.first_layer.dot(self.hidden_1_2))            \n",
    "\n",
    "            # Output error\n",
    "            second_layer_error = second_layer - self.translate_label[label]\n",
    "            second_layer_delta = second_layer_error * self.sigmoid_output_2_derivative(second_layer)\n",
    "\n",
    "            # Backpropagated error\n",
    "            first_layer_error = second_layer_delta.dot(self.hidden_1_2.T)\n",
    "            first_layer_delta = first_layer_error\n",
    "            self.hidden_1_2 -= self.first_layer.T.dot(second_layer_delta) * self.learning_rate\n",
    "\n",
    "            for index in review:\n",
    "                self.hidden_0_1[index] -= first_layer_delta[0] * self.learning_rate\n",
    "\n",
    "            if(second_layer >= 0.5 and label == 'POSITIVE'):\n",
    "                correct += 1\n",
    "            elif(second_layer < 0.5 and label == 'NEGATIVE'):\n",
    "                correct += 1\n",
    "            self.show_progress(i, len(reviews), correct)\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, reviews, testing_labels):\n",
    "        \"\"\" Test and don't update the weights \"\"\"\n",
    "        correct = 0\n",
    "        for i in range(len(reviews)):\n",
    "            pred = self.run(reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            self.show_progress(i, len(reviews), correct)\n",
    "    \n",
    "    def run(self, review, logger = False):\n",
    "        \"\"\" Evaluate a single review\"\"\"\n",
    "        self.first_layer *= 0\n",
    "        unique_indices = set()\n",
    "        for word in remove_stopwords(review):\n",
    "            if word in word2index.keys():\n",
    "                unique_indices.add(word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.first_layer += self.hidden_0_1[index]\n",
    "        second_layer = self.sigmoid(self.first_layer.dot(self.hidden_1_2))\n",
    "        out = second_layer[0]\n",
    "        threshold = 0\n",
    "        if logger:\n",
    "          print(out)\n",
    "          threshold = 0.05\n",
    "        if out >= 0.5 + threshold:\n",
    "            return \"POSITIVE\"\n",
    "        elif out < 0.5 - threshold:\n",
    "            return \"NEGATIVE\"\n",
    "        else:\n",
    "            return \"NEUTRAL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "RcYerV3FeeRl",
    "outputId": "f827bf10-effb-4c1e-f533-75b436f5e29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPOUT SPLIT Train: 17500, Test: 7500, TOTAL: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>one reason pixar has endured so well  and been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>i saw the film and i got screwed  because the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17502</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>a scanner darkly  minority report  blade runne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17503</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>what  s happening to rgv  he seems to repeat h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17504</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>i  ve seen this film in avant  premiere at ima...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels                                            reviews\n",
       "17500  POSITIVE  one reason pixar has endured so well  and been...\n",
       "17501  NEGATIVE  i saw the film and i got screwed  because the ...\n",
       "17502  POSITIVE  a scanner darkly  minority report  blade runne...\n",
       "17503  NEGATIVE  what  s happening to rgv  he seems to repeat h...\n",
       "17504  POSITIVE  i  ve seen this film in avant  premiere at ima..."
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividimos el dataset en 70% Training y 30% Testing\n",
    "DROPOUT_PARTITION = 0.7\n",
    "SPLIT_PART = int(len(df)*DROPOUT_PARTITION)\n",
    "df_train = df.iloc[:SPLIT_PART]\n",
    "df_test = df.iloc[SPLIT_PART:]\n",
    "print(\"DROPOUT SPLIT Train: %d, Test: %d, TOTAL: %d\"%(len(df_train), len(df_test), len(df_train)+len(df_test)))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "BzliWYOMGGFx",
    "outputId": "4c66f120-2df4-491f-846e-487cc828c88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Progress:0.0 %| Correct:1 | Accuracy:100.%\n",
      " - Progress:14.2 %| Correct:2028 | Accuracy:81.0%\n",
      " - Progress:28.5 %| Correct:4129 | Accuracy:82.5%\n",
      " - Progress:42.8 %| Correct:6277 | Accuracy:83.6%\n",
      " - Progress:57.1 %| Correct:8469 | Accuracy:84.6%\n",
      " - Progress:71.4 %| Correct:10629 | Accuracy:85.0%\n",
      " - Progress:85.7 %| Correct:12785 | Accuracy:85.2%\n",
      " - Progress:99.9 %| Correct:14934 | Accuracy:85.3%"
     ]
    }
   ],
   "source": [
    "net = SentimentReviewClassifier(learning_rate=0.02)\n",
    "net.train(df_train['reviews'],df_train['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KUIPK-nqGGF0",
    "outputId": "02bea834-e2e0-408a-8389-77549d8d1acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Progress:99.9 %| Correct:6453 | Accuracy:86.0%"
     ]
    }
   ],
   "source": [
    "# Cambiamos la forma de indexar por problemas en algún dato en el dataframe.\n",
    "net.test(df_test.iloc[:,1].values, df_test.iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sc7avjR9GGGE",
    "outputId": "ec3a923a-2890-4881-9dfb-d8c9dc1ad144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69505086]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a single review\n",
    "net.run('This a great film fantastic actors', logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "GpeJqAOyLwji"
   },
   "outputs": [],
   "source": [
    "#@title ### Inserta un texto para probar el analizador de sentimientos\n",
    "review = \"This movie is the best in the world\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "yzvyv-ukSEtC",
    "outputId": "b069da64-5dd1-491e-95a4-fefb9c59d632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review a analizar: \"This movie is the best in the world\"\n",
      "[0.6628306]\n",
      "La Review es: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print('Review a analizar: \"%s\"'%review)\n",
    "print('La Review es: %s'% net.run(review, logger=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R609eSV3b2-g"
   },
   "source": [
    "# CONCLUSIONES\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHrlm1PIJwOX"
   },
   "source": [
    "Como hemos podido observar, hemos obtenido resultados muy semejantes a los propuestos en el paper [1].\n",
    "\n",
    "Existen multitud de aproximaciones a un mismo problema.\n",
    "\n",
    "Determinar la polaridad positiva o negativa de una reseña se puede conseguir con modelos relativamente sencillos.\n",
    "\n",
    "Obtener datos más precisos, como qué tipo de sentimiento expresa, enfado, ira, amor, felicidad son un reto todavía en investigación.\n",
    "\n",
    "Técnicas muy similares propuestas en esta práctica se pueden utilizar para detectar reseñas fraudulentas, la dificultad está en conseguir un dataset etiquetado.\n",
    "\n",
    "Un analizador de sentimiento se puede simplificar a un clasificador de textos, en el que cada tópico es el sentimiento que queremos clasificar.\n",
    "\n",
    "Posibles mejoras, un mayor tratamiento en la reducción ruido, mediante la eliminación de palabras vacías aumentaría la precisión de nuestra red.\n",
    "\n",
    "Existen algoritmos más avanzados que posilemente den mejores resultados. Modelizar el corpus como word embeddings es una alternativa. Otra opción sería utilizar LSTM (Long short-term memory) [2]. Ambos sistemas tienen en cuenta las palabras que están cercanas y tenemos seguridad de que producirían mejores resultados.\n",
    "\n",
    "Aunque nosotros hemos tomado una vía muy rudimentaria para ir comentando y describiendo la metodología paso a paso, existen varias librerías que pueden simplificar nuestro código. Nosotros aconsejamos la utilización de estas librerías en sistemas reales. Algunas de estas librerías son: sklearn, pytorch, tensorflow, keras, nltk, scipy entre otras.\n",
    "\n",
    "Por último quremos destacar que este analizador de sentimientos funcionará bien con el dominio de películas en el idioma inglés, pero no sería el más adecuado para corpus de otros dominios, y por supuesto el resultado no sería fiable en el caso de clasificar documentos que no tengan ninguna palabra de nuestro vocabulario.\n",
    "\n",
    "De hecho al realizar esta prueba, se puede observar que el clasificador tiene un bias positio. Si no introduces ninguna palabra clasifica el texto como positivo con una confianza de 0.55."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Analizador-De-Sentimientos.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
